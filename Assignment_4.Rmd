---
title: "STAT418 Assignment 4"
author: "Rui Qiao"
date: "6/4/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SUSY Data Set
Detailed description: https://archive.ics.uci.edu/ml/datasets/SUSY

"This is a classification problem to distinguish between a signal process which produces supersymmetric particles and a background process which does not." 

"The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features). The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes."

The original data file ("SUSY.csv") contains 5,000,000 observations and has file size of 2.39 GB. Uploading that file to the H2O cluster will generate an error (java.lang.OutOfMemoryError: Java heap space). H2O recommends that the memory should be four times the size of the data. Since the memory of my computer is 8GB, I decided to use a much smaller dataset. 

I want to reduce the observations to 250,000. Besides deleting observations, I chose to delete all the low-level features in the dataset, leaving only high-level features as the predictor variables and the class label as the response. (An alternative way here to reduce file size can be spliting the file with 80% of observations.)

New file was produced and checked by running following codes in terminal (MacOS).
> cd Desktop/STAT418/SUSY
> cut -d, -f1,10- SUSY.csv > SUSY_high.csv
> split -l 500000 SUSY_high.csv SUSY_high_

This gives me a file named as "SUSY_high_aa" with 250,000 rows and 11 columns.

## Data Loading and Preparation

```{r data}
# h2o.shutdown()
library(h2o)

h2o.init(nthreads = -1)

col_names <- c("class", "MET_rel", "axial MET", "M_R", "M_TR_2", "R", 
               "MT2", "S_R", "M_Delta_R", "dPhi_r_b", "cos(theta_r1)")

SUSY <- h2o.importFile("/Users/Rachel/Desktop/STAT418/SUSY/SUSY_high_aa",
                       destination_frame = "SUSY", col.names = col_names)

y <- "class"
x <- setdiff(names(SUSY),y)

# For binary classification, response should be a factor
SUSY[y] <- as.factor(SUSY[y])
summary(SUSY[y])
# 1 for signal, 0 for background

# Split Data: train-valid-test
# Do not use cross-validation, dataset is large enough
parts <- h2o.splitFrame(SUSY, c(0.8,0.1), seed = 123)
SUSY_train <- parts[[1]]
SUSY_valid <- parts[[2]]
SUSY_test <- parts[[3]]
rm(parts)

```

## Neutral Networks

Try various architectures, tricks (initialization, regularization, momentum, adaptive learning rate etc.).Use early stopping. To reduce the Knit time of Rmarkdown file, the models are not shown in this filw but in a seperate file, NN_models.rmd (/NN_models.html).

By comparing AUC of 21 different Neutral Networks, the best model is found. The model uses almost all default settings. It uses Rectifier activation function ( outputs the sum of its weighted inputs, but clips all negative values to zero). No lasso or ridge regularization (l1, l2 = 0). Learning rate is default 0.005, momentum_stable = 0. It uses early stopping: If AUC does not improve in the last 2 scoring rounds, the process stops.

```{r NN2}
system.time({
  model_NN <- h2o.deeplearning(x = x, y = y, training_frame = SUSY_train, validation_frame = SUSY_valid,
            activation = "Rectifier", hidden = c(200,200), 
            adaptive_rate = FALSE, ## default: rate = 0.005, rate_decay = 1, momentum_stable = 0,
            epochs = 100, stopping_rounds = 2, stopping_metric = "AUC", stopping_tolerance = 0) 
})
h2o.performance(model_NN, SUSY_test)@metrics$AUC
```


## Hyperparameter optimization for GBMs with random search

```{r GBM}
hyper_params <- list(ntrees = 10000,  ## early stopping
                     max_depth = 5:15, 
                     min_rows = c(1,3,10,30,100),
                     learn_rate = c(0.01,0.03,0.1),  
                     learn_rate_annealing = c(0.99,0.995,1,1),
                     sample_rate = c(0.4,0.7,1,1),
                     col_sample_rate = c(0.7,1,1),
                     nbins = c(30,100,300),
                     nbins_cats = c(64,256,1024)
)

search_criteria <- list( strategy = "RandomDiscrete",
                        max_runtime_secs = 10*3600,
                        max_models = 100
)

system.time({
  GBM_grid <- h2o.grid(algorithm = "gbm", grid_id = "SUSY_GBM_grid",
                  x = x, y = y, training_frame = SUSY_train, 
                  validation_frame = SUSY_valid,
                  hyper_params = hyper_params,
                  search_criteria = search_criteria,
                  stopping_metric = "AUC", stopping_tolerance = 0.02, stopping_rounds = 2,
                  seed = 123)
})



GBM_auc <- h2o.getGrid(grid_id = "SUSY_GBM_grid", sort_by = "auc", decreasing = TRUE)
GBM_auc

model_GBM <- h2o.getModel(GBM_grid@model_ids[[1]])
summary(model_GBM)

h2o.auc(h2o.performance(model_GBM, SUSY_test))
```



## Ensembles

I want to find an ensemble of the best models with highest auc in each category (Logistic Regression, Random Forest, Neutral Networks, GBM). 

Member models must have been cross-validated using nfolds > 1, fold_assignment equal to Modulo, and keep_cross_validation_folds must be set to True. 

```{r}
nfolds = 5

system.time({
  m1 <- h2o.glm(x, y, training_frame = SUSY_train,
                      validation_frame = SUSY_valid,
                      family = "binomial",alpha = 1.0, 
                      lambda = 0,
                      seed = 123,
                      nfolds = nfolds,
                      fold_assignment = "Modulo",
                      keep_cross_validation_predictions = TRUE)
})

system.time({
  m2 <- h2o.randomForest(x = x, y = y, training_frame = SUSY_train, 
                 validation_frame = SUSY_valid, 
                 max_depth = 20, mtries = 3, ntrees =50,
                 seed = 123,
                 nfolds = nfolds,
                 fold_assignment = "Modulo",
                 keep_cross_validation_predictions = TRUE)
})

system.time({
  m3 <- h2o.deeplearning(x = x, y = y, training_frame = SUSY_train, validation_frame = SUSY_valid,
            epochs = 5,
            seed = 123,
            nfolds = nfolds,
            fold_assignment = "Modulo",
            keep_cross_validation_predictions = TRUE) 
})

system.time({
  m4 <- h2o.gbm(x, y, training_frame = SUSY_train,
                validation_frame = SUSY_valid,
                ntrees = 200,
                max_depth = 10, 
                learn_rate = 0.1,  
                nbins = 100,
                seed = 123,
                nfolds = nfolds,
                fold_assignment = "Modulo",
                keep_cross_validation_predictions = TRUE)
})

model_ENS <- h2o.stackedEnsemble(x = x, y = y, training_frame = SUSY_train,
                              base_models = list(m1@model_id, m2@model_id,
                                       m3@model_id, m4@model_id))
```

The GBM model with the highest auc has the following hyperparameters: col_sample_rate = 1.0, learn_rate = 0.3, learn_rate_annealing, max_depth = 15, min_rows= 100, nbins = 300, nbins_cats = 1024, ntrees = 10000 (#early stopping), sample_rate = 1.0.


## Summary Evaluation

### AUC table

```{r AUC}
s1 <- h2o.auc(h2o.performance(m1, SUSY_test))
s2 <- h2o.auc(h2o.performance(m2, SUSY_test))
s3 <- h2o.auc(h2o.performance(model_NN, SUSY_test))
s4 <- h2o.auc(h2o.performance(model_GBM, SUSY_test))
s5 <- h2o.auc(h2o.performance(model_ENS, SUSY_test))

a <-as.data.frame(cbind(c("LR", "RF", "NN","GBM","Ensembles"), c(s1,s2,s3,s4,s5)))
names(a) <- c("Model", "AUC")
a

```
Based on AUC comparison, the selected Neutral Network model is the best model.


### ROC plots

ROC plot is the plot of the true positive rate against the false positive rate for the different possible cutpoints of a diagnostic test. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity). The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.

```{r ROC, echo = FALSE}

plot(h2o.performance(m1, SUSY_test), main = "LR")

plot(h2o.performance(m2, SUSY_test), main = "RF")

plot(h2o.performance(model_NN, SUSY_test), main = "NN")

plot(h2o.performance(model_GBM, SUSY_test), main = "GBM")

plot(h2o.performance(model_ENS, SUSY_test), main = "Ensembles")

```

Comparing the ROC plots validates the previous selection of the best model based on AUC on test data.

## Model Performance (Selected models)

### Logistic Regression
```{r}
h2o.performance(m1, SUSY_test)

```


### Random Forest
```{r}
h2o.performance(m2, SUSY_test)

```


### Neutrual Network
```{r}
h2o.performance(model_NN, SUSY_test)
```


### GBM
```{r}
h2o.performance(model_GBM, SUSY_test)
```


### Ensembles
```{r}
h2o.performance(model_ENS, SUSY_test)
```

## Accuracy vs work and training time required

By comparing the accuracy in model performance and the work & training time of the models, it is clear that accuracy and time has negative relationship. Higher accuracy requires more work and training time. Less work and training time leads to low accuracy.


